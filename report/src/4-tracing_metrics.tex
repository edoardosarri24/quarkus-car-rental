\chapter{Tracing and Metrics}
\label{cap:tracing-metrics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tracing}
\label{cap:tracing}
In applicazioni con migliaia di micro servizi, seguire il workflow, cioè la cascata di chiamate, o valutare in che punto si è verificato un errore può essere molto complesso. Tramite il tracing non solo possiamo capire quale micro servizio viene chiamato e da chi, ma possiamo anche analizzare i tempi con cui ogni richiesta è stata servita, sia E2E che all'interno del singolo servizio.

Il funzionamento solitamente è abbastanza semplice: durante le varie chiamate viene trasmesso anche un ID (univoco) della traccia; il trasporto di questo ID avviene tramite una qualche funzionalità del protocollo di trasporto, che in HTTP è il relativo header. Quando la chiamata ritorna allora vengono unite le chiamate con lo stesso ID e viene generata la traccia.

\subsection{OpenTelemetry}
OpenTelemetry, noto anche come OTel, è un insieme di tool e API che permette di collezionare ed esportare le telemetrie di un'applicazione a microservizi. Oltre alle telemetrie in realtà possono anche essere gestite metriche e logs, ma in questo campo non è stabile e si preferisce usare altro (e.g., MicroMeter).

La vera forza di OpenTelemetry però non è tanto la raccolta di tracce, quando la definizione di OTLP, un protocollo stardard che permette di esportare le telemetrie verso vari e diversi backend (e.g., Jaeger per la visualizzazione).

\myskip

Per permettere il funzionamento di Opentelemtry con un qualche backend ci sono due possibili:
\begin{itemize}
    \item Direttamente: OpenTelemetry una volta raccolte le tracce le invia al backend in questione. È una strategia più semplice, ma non scala quando i backend aumentano e se dobbiamo fare un filtraggio delle tracce.
    \item Collector: I dati vanno all'Otel-Collector, che è il responsabile dell'invio e di operazioni intermedie (e.g., filtraggio, il batching e la gestione dei fallimenti). È la strategia più complesso, però in applicazioni che devono scalare è necessario.
\end{itemize}

\subsection{Collector}
Nella nostra applicazione è stato usato il collector perché si voleva inviare le tracce al backend di Jaeger e anche salvarle in un file per eseguire ulteriori analisi in un secondo momento.

La raccolta delle tracce con Quarkus diventa estremamente semplice: aggiungendo l'estensione \textit{quarkus-opentelemetry} il fraework instrumenta il codice e in automatico le telemetrie vengono collezionate. Una volta che le tracce sono raccolte vengono inviate al collector, dopo la configurazione ne Listing~\ref{lst:users-collector} (relativo a \texttt{users-service}), dove l'unica configurazione da spiegare è forse \texttt{quarkus.otel.traces.sampler=always\_on}: essa permette di non fare nessun campionamento delle chiamate, ma di considerarle tutte.

Il collector è un pod all'interno dell'ambiente Minikube che è stato deployato tramite una \href{https://github.com/edoardosarri24/quarkus-car-rental/blob/master/services/external-services/otel-collector}{configurazione helm}. Quando riceve le tracce le invia a due destinazioni: il pod di Jaeger per la visualizzazione e il file \textit{/traces/traces.json} che si trova nel PVC (Persinstant Volume Claim) \textit{pvc-traces} sempre all'interno del cluster. Quando eseguiremo l'analisi dei dati allora dovremmo eseguire l'analyser in un pod a cui è collegato questo PVC; in questo modo lo script esegue sui propri dati locali.

\begin{lstlisting}[caption=Jaeger configuration for \textit{users-service}, label=lst:users-collector]
# jaeger
quarkus.otel.service.name=users-service
quarkus.otel.exporter.otlp.traces.endpoint=http://otel-collector:4317
quarkus.otel.traces.sampler=always_on
\end{lstlisting}

\subsection{Jaeger}
Come abbiamo detto Jaeger è un backend per la visualizzazione del work di una chiamata in un'applicazione a microservizi. 

Come per i servizi terzi elencati nella Sezione~\ref{sec:servizi_esterni}, anche il backend Jaeger deve essere rilasciato nello stesso cluster Minikube di tutta l'applicazione. Anche in questo caso è stato utilizzato il chart Helm Jaeger~\cite{jaeger_helm}. Si può vedere il comando nel Listing~\ref{lst:jaeger-helm}.
\begin{lstlisting}[caption=Install Jaeger chart for \textit{users-service}, label=lst:jaeger-helm]
helm install jaeger jaegertracing/jaeger \
    --set allInOne.enabled=true \
    --set agent.enabled=false \
    --set collector.enabled=false \
    --set query.enabled=false \
    --set provisionDataStore.cassandra=false \
    --set storage.type=memory
\end{lstlisting}

\subsection{OTel workflow}
Fino a questo memomento in qusta sezione abbiamo parlato dei componenti in modo separato; vediamo adesso di mettere tutto insieme per capire come i vari pod interagiscono nella nostra applicazione.

Analiziamo il flusso mostrato in Figura~\ref{fig:otel_workflow}:
\begin{itemize}
    \item Viene chiamata una qualche API dell'applicaizone.
    \item OpenTelemtry raccoglio le telemetri e invia le tracce al collector, che eventualmente fa quale operazione.
    \item Il collector invia le tracce ai vari backend. Nel nostro caso Jaeger per la visualizzazione e il file system per l'analisi successiva.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/4-tracing_metrics/otel_workflow/otel_workflow.pdf}
    \caption{OpenTelemetry workflow.}
    \label{fig:otel_workflow}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Metrics}
Oltre a tracciare la sequenza delle varie chiamate, come abbiamo mostrato nel Capitolo~\ref{cap:tracing}, in un applicazione a micro servizi è importante anche essere in grado di catturare metriche, valori reali che espimono una qualche informazione.

In generale possiamo esporre metriche framework o business: le prime sono stardard fornite da un qualche framework e sono generali e valide in un qualunque ambito (e.g., carico della cpu, uso della memoria o richieste per secondo); le seconde sono metriche specifiche per il dominio dell'applicazione.

\myskip

La specifica MicroProfile per le metriche si chiama Metrics. Nonostante questo solitamente viene usata MicroMeter, un'implementazione che non segue la specifica MicroProfile. Quarkus permette in ogni caso di usare sia implemetazioni della specifica Metrics che MicroMeter, nonostante la seconda sia quella preferibile.

\subsection{Prometheus}
\label{sec:prometheus}
Promethues è un sistama open-source per il monitoraggio e allerta di applicazioni. Svolge tre compiti pricipali: raccoglie metriche, le memorizza e fornisce un endpoint utile per fare querying.

\myskip

In Quarkus utilizzare MicroMetrics e Prometheus insieme è molto semplice: dobbiamo solo aggiungere l'estensione \textit{quarkus-micrometer-registry-prometheus}: senza strumentalizzare il codice, MicroMeter inizia a raccogliere mentriche standard (e.g., utilizzo CPU e della memoria) nel formato richiesto da Prometheus, mentre quest'ultimo periodicamente interroga gli endpoint creati, raccogliendo e salvando tali metriche.

Dobbiamo a questo punto definire un modo per dire a Prometheus di raccogliere le metriche che MicroMetrics definisce. In Kubernetes questo metodo è detto \textit{ServiceMonitor} e può essere semplicemente implementato nel file di configurazione, come vediamo nel Listing~\ref{lst:users_prometheus}
\begin{lstlisting}[caption=Prometheus configuration for \textit{users-service}, label=lst:users_prometheus]
# prometheus and grafana
quarkus.kubernetes.prometheus.generate-service-monitor=true
quarkus.kubernetes.labels.release=prometheus
\end{lstlisting}

Infine serve installare nel cluster Minikube un container in cui esegue il server Prometheus. Questo è stato fatto, come nel resto del progetto, tramite Helm e il suo chart c. Questo non solo installa Promuethus, ma è un singolo pacchetto che comprende anche Grafana.

\subsection{Grafana}
Il sistema di query di Promethues permette di visualizzaare le metriche raccolte in forma testuale tramite terminale. Quando le metriche sono complesse, ma anche in generale, questo può non essere molto comodo.

Grafana è un backend che permette di visualizzare in modo grafico le metriche raccolte da Promethues, come si può vedere nella Figura~\ref{fig:pr_gr_flow}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\textwidth]{images/4-tracing_metrics/prom graf flow.pdf}
    \caption{Prometheus-Grafana Flow.}
    \label{fig:pr_gr_flow}
\end{figure}

Come dette nella Sezione~\ref{sec:prometheus} l'installazione all'interno del cluster Minikube è stata fatta con il chart di Helm \textit{kube-prometheus-stack}~\cite{prometheus_stack}, che comprende sia Prometheus che Grafana.