\chapter{Deployment}
In questo capitolo descriviamo il processo di deployment, più complesso di quello che ci aspettavamo. Nonostante infatti l'applicazione fosse già sviluppata in ogni suo aspetto, essa era predisposta per essere rilasciata su OpenShift e usare Quay.io come registry per le immagini.

Per questi motivi il file di configurazione Quarkus \textit{application.properties} sono stati complettamente rivisti e adattati alla nostra configurazione, che come abbiamo detto nel Capitolo~\ref{cap:introduzione} comprende Minikube come ambiente di deployment e Docker come runtime per i container.

\myskip

Per automattizzare il deployment, nella root directory del progetto è presente una cartella \textit{exec}, all'interno della quale è presente il file \href{https://github.com/edoardosarri24/quarkus-car-rental/blob/master/exec/deployment.sh}{\textit{deployment.sh}}. Se eseguito a partire dalla root directory e con il Docker demon in esecuzione, allora verrà creato un cluster Minikube con tutti i micro servizi e le loro dipendenze. Il corretto funzionamento di questo script è garantito su MacOS con ARM64.

All'interno dello script viene usato, per ogni micro servizio, il comando \texttt{quarkus build}. Questo permette di costruire l'immagine Docker in automatico, senza scrivere a mano il manifesto Kubernetes; l'unico requisito è l'utilizzo dell'estensione \textit{quarkus-container-image-docker}. Inoltre, cosa che in Minikube non è permesso, sarebbe consentito anche eseguire il deployment della nuova immagine appena costruita nell'ambiente di Deployment; questo può essere fatto tramite la configurazione \textit{quarkus.container-image.push=true}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Environment}
L'ambiente per il rilascio dell'applicazione è stato Minikube, un ambiente locale di sviluppo basato su Kubernetes. In questo modo è stato possibile testare il deployment in un ambiente simile a quello di produzione senza la necessità di risorse hardware dedicate. Come abbiamo detto, nonostante l'applicazione rilasciata su GitHub sia completa e configurata per un tipo di deployment, ci sono stati vari aspetti da considerare e che hanno richiesto attenzione e tempo:
\begin{itemize}
    \item \textbf{OpenShift} \\
        L'applicazione era stata pensata per essere distributa su OpenShift, la piattaforma per container basata su Kubernetes e sviluppata da Red Hat. Nonostante essa sia appunto Kubernetes-based, in Quarkus si richiede di specificare quale ambiente di deployment si intente utilizzare all'interno del file di configurazione \textit{application.properties}. \\
        Un esempio di configurazione corretta per Kubernetes, usando Docker come runtime di container, è quella mostrata nel Listing~\ref{lst:k8s_config}: questo è il contenuto del file di configurazione Quarkus per \textit{inventory-service}, ma generalizzando è lo stesso usato in tutti gli altri microservizi. Oltre ai comandi che sono naturalmente interpretabili, possiamo specificare alcuni concetti più complessi: \texttt{quarkus. \allowbreak container-image.build=true} permette di costruire l'immagine in automatico quando eseguiamo il comando \texttt{quarkus build}; \texttt{quarkus.kubernetes.service- \allowbreak type=NodePort} istruisce Kubernetes a creare un Service di tipo \textit{NodePort} (di default è \textit{ClusterIP}) e quindi accessibile anche dall'esterno del cluster (meno corretto in produzione di \textit{LoadBalancer}, ma giusto per test locali); \texttt{quarkus.kubernetes. \allowbreak image-pull-policy=Never} istruisce Kubernetes a far fallire l'avvio del pod se l'immagine Docker su cui si basa non è presente localmente (a differenza di \textit{Always} che la scarica sempre da un registry).
    \item \textbf{Registry} \\
        Una volta costruita l'immagine docker, il file di configurazione era progettato per eseguire il push su un registry, Quay.io, registry di Red Hat che fa parte dello stesso ecosistema di OpenShift. Siccome il nostro ambiente per il deployment era Minikube, sono state fatte delle modifiche anche in questo senso in modo da rilasciare l'immagine del microservizio all'interno di Minikube stesso. \\
        In questo scenario è fondamentale la configurazione \texttt{quarkus.container-image. \allowbreak push=false} all'interno del Listing~\ref{lst:k8s_config}.
    \item \textbf{Riferimenti} \\
        Usando un ambiente di produzione diversi (OpenShift vs Minikube) rispetto a quello per cui l'applicazione era stata pensata, si sono dovuti cambiare gli URL con cui un microservizio identificava le proprie dipendenze. Un esempio di quello che è stato fatto si trova nel Listing~\ref{lst:reference}, relativo al micro servizio \textit{rental-service}: i riferimenti per l'ambiente di sviluppo sono stati lasciati invariati, mentre sono state configurate correttamente le variabili d'ambiente all'interno del container in modo che sia presente il riferimento alla dipendenza usata.
\end{itemize}

\begin{lstlisting}[caption=Kubernetes and Docker Configuration, label=lst:k8s_config]
# container
quarkus.container-image.build=true
quarkus.container-image.push=false
quarkus.container-image.group=edoardosarri
quarkus.container-image.name=inventory-service
quarkus.container-image.tag=1.1
# kubernetes
quarkus.kubernetes.name=inventory-service
quarkus.kubernetes.deployment-target=kubernetes
quarkus.kubernetes.service-type=NodePort
quarkus.kubernetes.image-pull-policy=Never
\end{lstlisting}

\begin{lstlisting}[caption=Reference in different environments, label=lst:reference]
quarkus.rest-client.reservation.url=http://localhost:8081
quarkus.kubernetes.env.vars.quarkus-rest-client-reservation-url=http://reservation-service
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Servizi Esterni}
Come abbiamo già detto nella Sezione~\ref{sec:servizi_esterni}, i micro servizi business dell'applicazione utilizzano al loro interno altri servizi ausiliari. Questi devono eseguire all'interno dello stesso cluster Minikube e quindi devono essere in un qualche modo rilasciati.

La prima soluzione esplorata è stata farsi scrivere da un LLM il manifesto Kubernetes per l'ambiente Minikube. Nonostante questa fosse una soluzione funzionante, mi sembrava chiaro che ci potesse essere un'alternativa off-the-shelf che mi permettesse di non dover fare configrazioni manuali; l'alternativa adottata è stata Helm.

\myskip

L'unico servizio esterno per cui è stato generato (da un LLM, in particolare Gemini) il manifesto invece che usare un chart è stato MongoDB. Il problema, che non sono riuscito a risolvere, è stato quella della compatibilità tra la versione di MongoDB e quella di Minikube.

\subsection{Helm}
Helm è un gestore di pacchetti Kubernetes che semplifica il ciclo di vita delle applicazioni all'interno di Kubernetes.

Tramite i Chart possiamo definire, installare e aggiornare anche le applicazioni Kubernetes più complesse. Essi permettono infatti di non generare manifesti complessi e ridondanti a mano, ma di utilizzare una struttura predefinita e riutilizzabile. Nello stesso modo in cui esiste un database di immagini Docker, esiste un repository di Chart Helm, ArtifactHUB; oltre a fornirci molte applicazione, il repository ci fornisce tutte le istruzioni necessarie per configurazioni avanzate~\cite{artifacthub_mysql}.

\subsection{Deployment con Helm}
Per capire come Helm semplifica il deployment di servizi terzi da cui un nostro micro servizio dipende, prendiamo come esempio l'installazione del database MySQL, necessario per \textit{inventory-service}. Tramite un comando, seguito da pochi parametri, come si vede nel Listing~\ref{lst:helm_mysql}, viene rilasciato un pod MySQL nel cluster Minikube.
\begin{lstlisting}[caption=MySQL Helm chart, label=lst:helm_mysql]
helm install mysql-inventory bitnami/mysql \
    --set auth.rootPassword=root-pass \
    --set auth.database=mysql-inventory \
    --set auth.username=user \
    --set auth.password=pass
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Health}
Durante la fase di deployment, e in particolare durante il deployment automazzato con \textit{deployment.sh}, è stato notato che il servizio \textit{reservation-service} non funzionava correttamente. In particolare il pod partiva, andava in stato \textit{running} e \textit{ready}, ma non si riusciva a connetere al suo database PostgreSQL. Dopo numerose e lunghe investigazioni, analizzando i log dei pod, il problema è stato risolto eliminando manualmente il pod: Kubernetes riavvia una nuova istanza e i problemi di connesione vengono risolti.

Questo ci può portare ad affermare che, per qualche motivo, il pod necessità che il database PostgreSQL sia disponibile e pronto per accettare connessioni prima di avviarsi correttamente.

\subsection{MicroProfile Health}

Per evitare di riavviare il pod manualmente o di introdurre nello script del deployment una \textit{wait} è stato deciso di forzare il pod relativo a \textit{reservation-service} ad attendere la disponibilità del database prima di avvarsi.

Questo viene fatto tramite la specifica Health di MicroProfile implementata poi da SmallRye; essa estende le specifiche di Jakarta per adattare quest'ultima all'architettura dei micro servizi. Permette di esporre lo stato di un'applicazione, cioè un valore binario (a differenza delle metriche che sono valori numerici qualunque), che viene solitamente controllato dall'orchestratore che gestisce il ciclo di vita delle applicazione (e.g., Kubernetes) per decidere se un'applicazione è sana e può essere usata.

I tre maggiori controlli di salute che MicroProfile mette a disposizione sono \textit{Startup}, \textit{Readiness} e \textit{Liveness}~\cite{kubernetes_probes}: la prima verifica che l'applicazione sia stata avviata correttamente; la seconda controlla se l'applicazione è pronta a ricevere traffico; la terza monitora lo stato dell'applicazione durante il suo funzionamento. Se uno di questi controlli fallisce solitamente il pod viene riavviato.

\subsection{Implementazione}

Considerando che il nostro problema viene risolto facendo ripartire il pod del micro servizio una volta che il database è disponibile, l'Health Check utile è lo \textit{StartUp Check}. È stata definita la classe \href{https://github.com/edoardosarri24/quarkus-car-rental/blob/master/services/reservation-service/src/main/java/org/acme/reservation/health/DatabaseConnectionHealthCheck.java}{DatabaseConnectionHealthCheck.java}, annotata con l'annotazione \texttt{org.eclipse.microprofile.health.Startup}, che implementa una \texttt{SELECT} sul database: se il comando ha successo allora il check termina con successo e il pod è in esecuzione correttamente, altrimenti il pod viene fatto ripartire.

\myskip

Nella classe la connessione viene gestita da un oggetto di tipo \texttt{AgroalDataSource}\cite{quarkus_datasource}: \textit{DataSource} è un'interfaccia che rappresenta una connessione a un database standard in Java (JDBC); \textit{Agroal} è un'implementazione di questa interfaccia ed è quella che funziona meglio con Quarkus.

Quarkus permette con molta facilità di gestire le metriche di Health con le apposite estensioni~\cite{quarkus_health}. Basta infatti aggiungere \textit{quarkus-smallrye-health}, \textit{quarkus-agroal} e \textit{quarkus-jdbc-postgresql} e il tutto funiona completamente. L'unica cosa agiguntiva a cui dobbiamo pensare è a fornire l'url a cui l'oggetto \texttt{AgroalDataSource} deve connettersi; questo è facilmente configurabile nell'\textit{application.properties}, come si vede dal Listing~\ref{lst:db_config}. Sono state anche aggiunte alcune configurazioni~\cite{quarkus_all_config} per gestire dopo quanti tentativi falliti riavviare il pod e, per evitare troppi riavvii, il tempo dopo quanto iniziare a controllare.
\begin{lstlisting}[caption=JDBC configuration., label=lst:db_config]
# health
quarkus.kubernetes.startup-probe.initial-delay=60
quarkus.kubernetes.startup-probe.failure-threshold=1
quarkus.datasource.jdbc.url=jdbc:postgresql://localhost:5432/reservation
%prod.quarkus.datasource.jdbc.url=jdbc:postgresql://postgresql-reservation:5432/reservation
\end{lstlisting}